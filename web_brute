#!/usr/bin/env ruby
#/ Usage: web_brute [options] ...
#/
#/ This script ...

$stderr.sync = true

require 'optparse'
require 'typhoeus'
require 'digest'

METHOD = 'GET'
TARGET = "http://172.22.10.117:8080"
HOST = nil
DIR_LIST = File.expand_path(File.dirname(__FILE__) + "/wordlists/web/raft-large-directories-lowercase.txt"
FILE_LIST = File.expand_path(File.dirname(__FILE__) + "/wordlists/web/raft-large-files-lowercase-noext.txt"
USER_AGENTS = File.readlines(File.expand_path(File.dirname(__FILE__) + "/wordlists/user_agents.txt")).each {|line| line.chomp!}
MAX_CONCURRENCY = 200
STATUS_CODES = [200, 301, 307, 401]
IGNORE_STATUS_CODES = [404,500]
IGNORE_CONTENT_LENGTH = []
RECURSE_DEPTH = 5
EXTENSIONS = ['htm', 'html', 'asp', 'aspx', 'jsp', 'php']

# argument default values 

input = nil

# parse arguments

file = __FILE__
ARGV.options do |opts|
    opts.on("-i", "--input FILE", String) { |val| input = val }
    opts.on_tail("-h", "--help")          { exec "grep ^#/<'#{file}'|cut -c4-" }
    opts.parse!
end

def brute(target, hostname, wordlist)
    results = []

    Typhoeus::Config.user_agent = USER_AGENTS.sample
    hydra = Typhoeus::Hydra.new(max_concurrency: MAX_CONCURRENCY)

    uri = URI(target)
    vhost = hostname || uri.host
    resolve = Ethon::Curl.slist_append(nil, "#{vhost}:#{uri.port}:#{uri.host}")

    wordlist.each do |word|
        url = "#{target.chomp('/')}/#{word}"

        request = Typhoeus::Request.new(
            url,
            resolve: resolve,
            method: METHOD,
            followlocation: false,
            connecttimeout: 5,
            timeout: 10,
            ssl_verifyhost: 0,
            ssl_verifypeer: false
        )

        request.on_complete do |response|
            if response.timed_out?
                STDERR.puts "#{url},TMO,#{METHOD},0"
            elsif response.code.zero?
                # Could not get an http response, something's wrong.
                STDERR.puts "#{url},ERR,#{METHOD},0"
            else
                content_length = response.headers['content-length'].nil? ? response.body.size : response.headers['content-length']

                next unless STATUS_CODES.empty? or STATUS_CODES.include? response.code
                next if IGNORE_CONTENT_LENGTH.include? content_length
                next if IGNORE_STATUS_CODES.include? response.code

                hash = METHOD == 'GET' ? Digest::MD5.hexdigest(response.body) : String.new

                puts "#{url},#{METHOD},#{response.code},#{content_length},#{hash},#{vhost}"
                results << url
            end
        end

        hydra.queue request
    end

    hydra.run

    results
end

# 1) Test directory list

dir_results = Set.new

dirs = IO.readlines(DIR_LIST).map do |word|
    word = word.strip                       # remove newline and whitespace characters
    word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
    line += '/' if line[-1,1] != '/'        # add trailing slash if it doesn't exist
    word
end

urls = Set.new
urls << TARGET
depth = 0

loop do
    results = Set.new
    depth += 1

    urls.each do |url|
        results += brute(url, HOST, dirs)
    end

    break if results.empty?
    break if depth == RECURSE_DEPTH

    dir_results += results
    urls = results
end

# 2) Test file list with no extensions

file_results = Set.new

files = IO.readlines(FILE_LIST).map do |word|
    word = word.strip                       # remove newline and whitespace characters
    word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
    word
end

dir_results << TARGET

dir_results.each do |url|
    file_results += brute(url, HOST, files)
end

# 3) Test file list with custom extensions (asp, aspx, htm, html, jsp, php).

files = IO.readlines(FILE_LIST).flat_map do |word|
    word = word.strip                       # remove newline and whitespace characters
    word = word[1..-1] if word[0,1] == '/'  # remove leading slash if it exists
    combi = Array.new
    EXTENSIONS.each do |ext|
        combi << word + '.' + ext
    end
    combi
end

dir_results << TARGET

dir_results.each do |url|
    file_results += brute(url, HOST, files)
end

# 4) Test file list with variant extensions (bac, BAC, backup, BACKUP, bak, BAK, bk, conf, cs, csproj, gz, inc, INC, java, log, lst, old, OLD, orig, ORIG, sav, save, tar, temp, tmp, TMP, vb, vbproj, zip, $$$, -OLD, -old, 0, 1, ~1, ~bk).
# 5) Test file list with observed extensions (if different from custom/variant), except for (bz2, class, com, doc, docx, exe, gif, gz, jar, jpeg, jpg, mp3, mpeg, mpg, pdf, pif, png, ram, rar, scr, snp, swf, tgz, tif, wav, xls, xlsx, xml, zip).
# 6) Test observed files with custom extensions.
# 7) Test observed files with variant extensions.
# 8) Test observed files with observed extensions, except for (bz2, class, com, doc, docx, exe, gif, gz, jar, jpeg, jpg, mp3, mpeg, mpg, pdf, pif, png, ram, rar, scr, snp, swf, tgz, tif, wav, xls, xlsx, xml, zip).

# Options:
# A) Extract links
# B) Files only, directories only, both files and directories
# C) Recurse sub-directories (to depth: X)

# Input: List of URLs

# Group URLs by IP + hostname, so that enumeration does not happen twice
# Split each batch of tests into tasks, and add to a queue
# Tasks can then be pushed to top or bottom of queue as appropiate